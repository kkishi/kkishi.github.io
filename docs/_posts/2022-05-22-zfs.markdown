---
layout: post
title: ZFS
---

## 作業ログ

### 2023-08-19

新しくPCを組んでtank-mirrorを常時mountするようにした。

ファイルを/tankにコピーしてさらにZFS sendするには、~/projects/に https://github.com/kkishi/misc がcloneされているとして:

```
keisuke@saginomiya:~/projects/misc/import$ ./gopro.sh
keisuke@saginomiya:~/projects/misc/import$ ./gh5.sh
keisuke@saginomiya:~/projects/misc/zfs_sync$ go run remote.go --addr=10.0.0.13
```

### 2023-05-23

#### 別のマシンにZFSでフォーマットされたHDDをUSB接続してSSH越しにzfs send/receiveをする

まず受信側でzpoolをimportする。このマシンにこのHDDを接続するのは初めてだったが特に設定は必要なく次のコマンドでimportできた。

```sh
keisuke@hibarigaoka:~$ sudo zpool import tank-mirror
```

SSH越しにコマンドを実行するときにパスワードを要求されると不便なので、receiveに必要なパーミッションを与えておく。

```sh
keisuke@hibarigaoka:~$ sudo zfs allow keisuke create,mount,receive tank-mirror/photos
```

ついでに送信側でもsendに必要なパーミッションを与える。

```sh
keisuke@saginomiya:~$ sudo zfs allow keisuke send tank/photos
```

次のようなコマンドでsnapshotを送信できる。

```sh
keisuke@saginomiya:~$ zfs send -v -i tank/photos@2023-05-08-11:04 tank/photos@2023-05-23-11:08 | ssh keisuke@10.0.0.2 'zfs receive -v -F tank-mirror/photos'
```

#### 新しいdatasetをreceiveする(自動的に受信側でdatasetをcreateする)

```sh
keisuke@hibarigaoka:~$ sudo zfs allow keisuke receive,create,mount tank-mirror
```

```sh
keisuke@saginomiya:~$ sudo zfs create tank/media
keisuke@saginomiya:~$ sudo zfs snapshot tank/media@2023-05-24-00:40
keisuke@saginomiya:~$ zfs send -v tank/media@2023-05-24-00:40 | ssh keisuke@10.0.0.2 'zfs receive -v tank-mirror/media'
```

#### 基本的なコマンド

```sh
keisuke@saginomiya:~$ zpool status tank -v
  pool: tank
 state: ONLINE
  scan: scrub repaired 0B in 01:58:43 with 0 errors on Sun May 14 02:22:46 2023
config:

	NAME                        STATE     READ WRITE CKSUM
	tank                        ONLINE       0     0     0
	  raidz2-0                  ONLINE       0     0     0
	    wwn-0x50014ee2bf21c772  ONLINE       0     0     0
	    wwn-0x50014ee2bf218152  ONLINE       0     0     0
	    wwn-0x50014ee269cb9a72  ONLINE       0     0     0
	    wwn-0x50014ee214768e83  ONLINE       0     0     0
	    wwn-0x50014ee269cba7a6  ONLINE       0     0     0
	    wwn-0x50014ee269cbc494  ONLINE       0     0     0
	    wwn-0x50014ee269cb8402  ONLINE       0     0     0
	    wwn-0x50014ee2bf21c67f  ONLINE       0     0     0

errors: No known data errors
```

```sh
keisuke@saginomiya:~$ zfs list tank -r
NAME          USED  AVAIL     REFER  MOUNTPOINT
tank         5.29T  15.3T      114G  /tank
tank/media    748G  15.3T      743G  /tank/media
tank/photos  4.45T  15.3T     3.81T  /tank/photos
```

```sh
keisuke@saginomiya:~$ zfs list tank -t snapshot -r
NAME                           USED  AVAIL     REFER  MOUNTPOINT
tank/media@2023-05-24-00:40   4.85G      -      743G  -
tank/media@2023-05-24-08:54    239K      -      743G  -
tank/photos@2022-05-22         649G      -     3.43T  -
tank/photos@2023-01-30        12.8M      -     3.52T  -
tank/photos@2023-04-22        3.65M      -     3.65T  -
tank/photos@2023-04-23-11:10  3.67M      -     3.69T  -
tank/photos@2023-05-04-09:23  13.1M      -     3.72T  -
tank/photos@2023-05-08-11:04  12.9M      -     3.81T  -
tank/photos@2023-05-23-11:08  3.27M      -     3.81T  -
```




### 2022-12-30

昨日実行したscrubが終わった後statusを調べたらREADとWRITEでエラーが出ていた。

```sh
            wwn-0x50014ee269cb8402  ONLINE       6     1     0
```

`/var/log/kern.log`にもエラーが表示されている。

```sh
$ cat /var/log/kern.log
...
Dec 29 11:45:56 saginomiya kernel: [70761.593330] ata2.00: exception Emask 0x0 SAct 0x72000 SErr 0xd0000 action 0x6 frozen
Dec 29 11:45:56 saginomiya kernel: [70761.593339] ata2: SError: { PHYRdyChg CommWake 10B8B }
Dec 29 11:45:56 saginomiya kernel: [70761.593343] ata2.00: failed command: WRITE FPDMA QUEUED
Dec 29 11:45:56 saginomiya kernel: [70761.593345] ata2.00: cmd 61/10:68:98:35:00/00:00:22:01:00/40 tag 13 ncq dma 8192 out
Dec 29 11:45:56 saginomiya kernel: [70761.593345]          res 40/00:01:06:4f:c2/00:00:00:00:00/00 Emask 0x4 (timeout)
Dec 29 11:45:56 saginomiya kernel: [70761.593351] ata2.00: status: { DRDY }
Dec 29 11:45:56 saginomiya kernel: [70761.593353] ata2.00: failed command: READ FPDMA QUEUED
Dec 29 11:45:56 saginomiya kernel: [70761.593355] ata2.00: cmd 60/28:80:88:1a:a0/00:00:69:01:00/40 tag 16 ncq dma 20480 in
Dec 29 11:45:56 saginomiya kernel: [70761.593355]          res 40/00:ff:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)
Dec 29 11:45:56 saginomiya kernel: [70761.593359] ata2.00: status: { DRDY }
Dec 29 11:45:56 saginomiya kernel: [70761.593361] ata2.00: failed command: READ FPDMA QUEUED
Dec 29 11:45:56 saginomiya kernel: [70761.593362] ata2.00: cmd 60/30:88:b0:1a:a0/00:00:69:01:00/40 tag 17 ncq dma 24576 in
Dec 29 11:45:56 saginomiya kernel: [70761.593362]          res 40/00:01:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)
Dec 29 11:45:56 saginomiya kernel: [70761.593366] ata2.00: status: { DRDY }
Dec 29 11:45:56 saginomiya kernel: [70761.593367] ata2.00: failed command: READ FPDMA QUEUED
Dec 29 11:45:56 saginomiya kernel: [70761.593368] ata2.00: cmd 60/28:90:e0:1a:a0/00:00:69:01:00/40 tag 18 ncq dma 20480 in
Dec 29 11:45:56 saginomiya kernel: [70761.593368]          res 40/00:01:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)
Dec 29 11:45:56 saginomiya kernel: [70761.593372] ata2.00: status: { DRDY }
Dec 29 11:45:56 saginomiya kernel: [70761.593375] ata2: hard resetting link
Dec 29 11:45:57 saginomiya kernel: [70762.065012] ata2: SATA link up 6.0 Gbps (SStatus 133 SControl 300)
Dec 29 11:45:57 saginomiya kernel: [70762.067355] ata2.00: configured for UDMA/133
Dec 29 11:45:57 saginomiya kernel: [70762.067453] sd 1:0:0:0: [sde] tag#13 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=32s
Dec 29 11:45:57 saginomiya kernel: [70762.067457] sd 1:0:0:0: [sde] tag#13 Sense Key : Illegal Request [current]
Dec 29 11:45:57 saginomiya kernel: [70762.067459] sd 1:0:0:0: [sde] tag#13 Add. Sense: Unaligned write command
Dec 29 11:45:57 saginomiya kernel: [70762.067461] sd 1:0:0:0: [sde] tag#13 CDB: Write(16) 8a 00 00 00 00 01 22 00 35 98 00 00 00 10 00 00
Dec 29 11:45:57 saginomiya kernel: [70762.067462] blk_update_request: I/O error, dev sde, sector 4865406360 op 0x1:(WRITE) flags 0x700 phys_seg 2 prio class 0
Dec 29 11:45:57 saginomiya kernel: [70762.067469] zio pool=tank vdev=/dev/disk/by-id/wwn-0x50014ee269cb8402-part1 error=5 type=2 offset=2491087007744 size=8192 flags=180880
Dec 29 11:45:57 saginomiya kernel: [70762.067486] sd 1:0:0:0: [sde] tag#16 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=56s
Dec 29 11:45:57 saginomiya kernel: [70762.067487] sd 1:0:0:0: [sde] tag#16 Sense Key : Illegal Request [current]
Dec 29 11:45:57 saginomiya kernel: [70762.067488] sd 1:0:0:0: [sde] tag#16 Add. Sense: Unaligned write command
Dec 29 11:45:57 saginomiya kernel: [70762.067490] sd 1:0:0:0: [sde] tag#16 CDB: Read(16) 88 00 00 00 00 01 69 a0 1a 88 00 00 00 28 00 00
Dec 29 11:45:57 saginomiya kernel: [70762.067490] blk_update_request: I/O error, dev sde, sector 6067067528 op 0x0:(READ) flags 0x700 phys_seg 1 prio class 0
Dec 29 11:45:57 saginomiya kernel: [70762.067494] zio pool=tank vdev=/dev/disk/by-id/wwn-0x50014ee269cb8402-part1 error=5 type=1 offset=3106337525760 size=20480 flags=1808b0
Dec 29 11:45:57 saginomiya kernel: [70762.067499] sd 1:0:0:0: [sde] tag#17 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=56s
Dec 29 11:45:57 saginomiya kernel: [70762.067501] sd 1:0:0:0: [sde] tag#17 Sense Key : Illegal Request [current]
Dec 29 11:45:57 saginomiya kernel: [70762.067502] sd 1:0:0:0: [sde] tag#17 Add. Sense: Unaligned write command
Dec 29 11:45:57 saginomiya kernel: [70762.067503] sd 1:0:0:0: [sde] tag#17 CDB: Read(16) 88 00 00 00 00 01 69 a0 1a b0 00 00 00 30 00 00
Dec 29 11:45:57 saginomiya kernel: [70762.067504] blk_update_request: I/O error, dev sde, sector 6067067568 op 0x0:(READ) flags 0x700 phys_seg 1 prio class 0
Dec 29 11:45:57 saginomiya kernel: [70762.067506] zio pool=tank vdev=/dev/disk/by-id/wwn-0x50014ee269cb8402-part1 error=5 type=1 offset=3106337546240 size=24576 flags=1808b0
Dec 29 11:45:57 saginomiya kernel: [70762.067509] sd 1:0:0:0: [sde] tag#18 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=56s
Dec 29 11:45:57 saginomiya kernel: [70762.067510] sd 1:0:0:0: [sde] tag#18 Sense Key : Illegal Request [current]
Dec 29 11:45:57 saginomiya kernel: [70762.067511] sd 1:0:0:0: [sde] tag#18 Add. Sense: Unaligned write command
Dec 29 11:45:57 saginomiya kernel: [70762.067512] sd 1:0:0:0: [sde] tag#18 CDB: Read(16) 88 00 00 00 00 01 69 a0 1a e0 00 00 00 28 00 00
Dec 29 11:45:57 saginomiya kernel: [70762.067513] blk_update_request: I/O error, dev sde, sector 6067067616 op 0x0:(READ) flags 0x700 phys_seg 1 prio class 0
Dec 29 11:45:57 saginomiya kernel: [70762.067515] zio pool=tank vdev=/dev/disk/by-id/wwn-0x50014ee269cb8402-part1 error=5 type=1 offset=3106337570816 size=20480 flags=1808b0
Dec 29 11:45:57 saginomiya kernel: [70762.067519] ata2: EH complete
```

`ata2.00: failed command: WRITE FPDMA QUEUED`で検索すると、いくつかのページでHDDとPCの接続を調べろとある。確かに心当たりがある（最近グラフィックスカードを設置するためにSATAケーブルを抜き差しした）。

接続を調べると、現在エラーが出ている`wwn-0x50014ee269cb8402`は一番下、以前エラーが出ていた`wwn-0x50014ee269cba7a6`は下から二番目にある。マザーボードとの接続を調べるとどちらも同じ端子に接続されている。どうやらグラフィックスカードに圧迫されてケーブルの接触が悪くなっていたらしい。ケーブルの取り回しを改善して再度試す。しかし、エラーが出続ける。

仕方がないのでHBA(9211-8iが4ポート空いていた)に接続し直すとエラーが止まった。マザーボード付属のSATA端子が4つとも壊れてしまったかも。詳しいことは後で調べるとして当面はHBAにHDDを全て接続することにする。

### 2022-12-29

Ubuntu 22.04 にアップデートしたら bpool や rpool がドックに表示されるようになった([例](https://askubuntu.com/questions/1440112/how-to-remove-rpool-and-bpool-ssd-icons-from-ubuntu-22-10-dock))。

<https://bugs.launchpad.net/ubuntu/+source/gnome-shell-extension-ubuntu-dock/+bug/1967174>を参考にして以下のコマンドを実行すればよい。

```sh
$ gsettings set org.gnome.shell.extensions.dash-to-dock show-mounts false
```

### 2022-12-28

ふと`zpool status`を確認したらチェックサムエラーが訂正されていた。

```sh
$ zpool status tank
  pool: tank
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
        attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the errors
        using 'zpool clear' or replace the device with 'zpool replace'.
   see: https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-9P
  scan: resilvered 39.0M in 00:00:06 with 0 errors on Wed Dec 28 16:06:56 2022
config:

        NAME                        STATE     READ WRITE CKSUM
        tank                        ONLINE       0     0     0
          raidz2-0                  ONLINE       0     0     0
            wwn-0x50014ee2bf21c772  ONLINE       0     0     0
            wwn-0x50014ee2bf218152  ONLINE       0     0     0
            wwn-0x50014ee269cb9a72  ONLINE       0     0     0
            wwn-0x50014ee214768e83  ONLINE       0     0     0
            wwn-0x50014ee269cba7a6  ONLINE       0     0     1
            wwn-0x50014ee269cbc494  ONLINE       0     0     0
            wwn-0x50014ee269cb8402  ONLINE       0     0     1
            wwn-0x50014ee2bf21c67f  ONLINE       0     0     0

errors: No known data errors
```

せっかくなので`zpool clear`をする前に少し調べてみる。

<https://www.reddit.com/r/zfs/comments/o4lb3j/zfs_unrecoverable_error_how_to_know_for_sure_what/>を参考にして`/var/log/kern.log`を見ると、確かに`wwn-0x50014ee269cba7a6`と`wwn-0x50014ee269cb8402`に関連するエラーメッセージが見つかる。

```sh
$ cat /var/log/kern.log
...
Dec 25 19:17:35 saginomiya kernel: [222805.761773] ata2.00: exception Emask 0x0 SAct 0x4009c30c SErr 0xd0000 action 0x6 frozen
Dec 25 19:17:35 saginomiya kernel: [222805.761783] ata2: SError: { PHYRdyChg CommWake 10B8B }
Dec 25 19:17:35 saginomiya kernel: [222805.761787] ata2.00: failed command: READ FPDMA QUEUED
Dec 25 19:17:35 saginomiya kernel: [222805.761789] ata2.00: cmd 60/28:10:00:c0:1d/00:00:6f:00:00/40 tag 2 ncq dma 20480 in
Dec 25 19:17:35 saginomiya kernel: [222805.761789]          res 40/00:01:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)
Dec 25 19:17:35 saginomiya kernel: [222805.761795] ata2.00: status: { DRDY }
Dec 25 19:17:35 saginomiya kernel: [222805.761797] ata2.00: failed command: WRITE FPDMA QUEUED
Dec 25 19:17:35 saginomiya kernel: [222805.761799] ata2.00: cmd 61/08:18:48:39:00/00:00:79:01:00/40 tag 3 ncq dma 4096 out
Dec 25 19:17:35 saginomiya kernel: [222805.761799]          res 40/00:01:06:4f:c2/00:00:00:00:00/00 Emask 0x4 (timeout)
Dec 25 19:17:35 saginomiya kernel: [222805.761804] ata2.00: status: { DRDY }
...
Dec 25 19:17:35 saginomiya kernel: [222806.237488] ata2: SATA link up 6.0 Gbps (SStatus 133 SControl 300)
Dec 25 19:17:35 saginomiya kernel: [222806.239895] ata2.00: configured for UDMA/133
Dec 25 19:17:35 saginomiya kernel: [222806.239981] sd 2:0:0:0: [sde] tag#2 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=32s
Dec 25 19:17:35 saginomiya kernel: [222806.239985] sd 2:0:0:0: [sde] tag#2 Sense Key : Illegal Request [current]
Dec 25 19:17:35 saginomiya kernel: [222806.239987] sd 2:0:0:0: [sde] tag#2 Add. Sense: Unaligned write command
Dec 25 19:17:35 saginomiya kernel: [222806.239989] sd 2:0:0:0: [sde] tag#2 CDB: Read(16) 88 00 00 00 00 00 6f 1d c0 00 00 00 00 28 00 00
Dec 25 19:17:35 saginomiya kernel: [222806.239990] blk_update_request: I/O error, dev sde, sector 1864220672 op 0x0:(READ) flags 0x700 phys_seg 1 prio class 0
Dec 25 19:17:35 saginomiya kernel: [222806.239999] zio pool=tank vdev=/dev/disk/by-id/wwn-0x50014ee269cb8402-part1 error=5 type=1 offset=954479935488 size=20480 flags=180880
Dec 25 19:17:35 saginomiya kernel: [222806.240018] sd 2:0:0:0: [sde] tag#3 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=30s
Dec 25 19:17:35 saginomiya kernel: [222806.240020] sd 2:0:0:0: [sde] tag#3 Sense Key : Illegal Request [current]
Dec 25 19:17:35 saginomiya kernel: [222806.240022] sd 2:0:0:0: [sde] tag#3 Add. Sense: Unaligned write command
Dec 25 19:17:35 saginomiya kernel: [222806.240023] sd 2:0:0:0: [sde] tag#3 CDB: Write(16) 8a 00 00 00 00 01 79 00 39 48 00 00 00 08 00 00
Dec 25 19:17:35 saginomiya kernel: [222806.240024] blk_update_request: I/O error, dev sde, sector 6325025096 op 0x1:(WRITE) flags 0x700 phys_seg 1 prio class 0
Dec 25 19:17:35 saginomiya kernel: [222806.240029] zio pool=tank vdev=/dev/disk/by-id/wwn-0x50014ee269cb8402-part1 error=5 type=2 offset=3238411800576 size=4096 flags=180880
...
Dec 25 20:32:02 saginomiya kernel: [227272.369557] zio pool=tank vdev=/dev/disk/by-id/wwn-0x50014ee269cba7a6-part1 error=5 type=2 offset=3238433095680 size=4096 flags=180880
Dec 25 20:32:02 saginomiya kernel: [227272.369563] sd 6:0:0:0: [sdg] tag#17 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=30s
Dec 25 20:32:02 saginomiya kernel: [227272.369564] sd 6:0:0:0: [sdg] tag#17 Sense Key : Illegal Request [current]
Dec 25 20:32:02 saginomiya kernel: [227272.369566] sd 6:0:0:0: [sdg] tag#17 Add. Sense: Unaligned write command
Dec 25 20:32:02 saginomiya kernel: [227272.369567] sd 6:0:0:0: [sdg] tag#17 CDB: Write(16) 8a 00 00 00 00 01 79 00 db b8 00 00 00 08 00 00
Dec 25 20:32:02 saginomiya kernel: [227272.369568] blk_update_request: I/O error, dev sdg, sector 6325066680 op 0x1:(WRITE) flags 0x700 phys_seg 1 prio class 0
Dec 25 20:32:02 saginomiya kernel: [227272.369570] zio pool=tank vdev=/dev/disk/by-id/wwn-0x50014ee269cba7a6-part1 error=5 type=2 offset=3238433091584 size=4096 flags=180880
Dec 25 20:32:02 saginomiya kernel: [227272.369574] zio pool=tank vdev=/dev/disk/by-id/wwn-0x50014ee269cba7a6-part1 error=5 type=1 offset=1108155359232 size=24576 flags=180880
Dec 25 20:32:02 saginomiya kernel: [227272.369577] ata6: EH complete
```
(`~/misc/kern.log`に`Do 29 Dez 2022 09:32:50 CET`時点でのログを保存した)

ブラウザ履歴によると、2022-12-25 7:11 pm ごろに PhotoPrism で Library > Complete Rescan をしている（Dockerイメージを更新したため）。それに関連したデータアクセスで問題が発生したらしい。

SMARTを調べてみたが特に問題は無さそう。

```sh
$ sudo skdump /dev/disk/by-id/wwn-0x50014ee269cba7a6
Device: sat16:/dev/disk/by-id/wwn-0x50014ee269cba7a6
Type: 16 Byte SCSI ATA SAT Passthru
Size: 3815447 MiB
Model: [WDC WD40EFZX-68AWUN0]
Serial: [WD-WXB2D71HTZS9]
Firmware: [81.00B81]
SMART Available: yes
Quirks:
Awake: yes
SMART Disk Health Good: yes
Off-line Data Collection Status: [Off-line data collection activity was never started.]
Total Time To Complete Off-Line Data Collection: 43740 s
Self-Test Execution Status: [The previous self-test routine completed without error or no self-test has ever been run.]
Percent Self-Test Remaining: 0%
Conveyance Self-Test Available: no
Short/Extended Self-Test Available: yes
Start Self-Test Available: yes
Abort Self-Test Available: yes
Short Self-Test Polling Time: 2 min
Extended Self-Test Polling Time: 464 min
Conveyance Self-Test Polling Time: 0 min
Bad Sectors: 0 sectors
Powered On: 9.8 months
Power Cycles: 28
Average Powered On Per Power Cycle: 10.5 days
Temperature: 35.0 C
Attribute Parsing Verification: Good
Overall Status: GOOD
ID# Name                        Value Worst Thres Pretty      Raw            Type    Updates Good Good/Past
  1 raw-read-error-rate         200   200    51   0           0x000000000000 prefail online  yes  yes
  3 spin-up-time                229   223    21   3.5 s       0xbc0d00000000 prefail online  yes  yes
  4 start-stop-count            100   100     0   28          0x1c0000000000 old-age online  n/a  n/a
  5 reallocated-sector-count    200   200   140   0 sectors   0x000000000000 prefail online  yes  yes
  7 seek-error-rate             100   253     0   0           0x000000000000 old-age online  n/a  n/a
  9 power-on-hours               91    91     0   9.8 months  0x881b00000000 old-age online  n/a  n/a
 10 spin-retry-count            100   253     0   0           0x000000000000 old-age online  n/a  n/a
 11 calibration-retry-count     100   253     0   0           0x000000000000 old-age online  n/a  n/a
 12 power-cycle-count           100   100     0   28          0x1c0000000000 old-age online  n/a  n/a
192 power-off-retract-count     200   200     0   16          0x100000000000 old-age online  n/a  n/a
193 load-cycle-count            200   200     0   2503        0xc70900000000 old-age online  n/a  n/a
194 temperature-celsius-2       115   104     0   35.0 C      0x230000000000 old-age online  n/a  n/a
196 reallocated-event-count     200   200     0   0           0x000000000000 old-age online  n/a  n/a
197 current-pending-sector      200   200     0   0 sectors   0x000000000000 old-age online  n/a  n/a
198 offline-uncorrectable       100   253     0   0 sectors   0x000000000000 old-age offline n/a  n/a
199 udma-crc-error-count        200   200     0   0           0x000000000000 old-age online  n/a  n/a
200 multi-zone-error-rate       100   253     0   0           0x000000000000 old-age offline n/a  n/a

$ sudo skdump /dev/disk/by-id/wwn-0x50014ee269cb8402
Device: sat16:/dev/disk/by-id/wwn-0x50014ee269cb8402
Type: 16 Byte SCSI ATA SAT Passthru
Size: 3815447 MiB
Model: [WDC WD40EFZX-68AWUN0]
Serial: [WD-WXB2D71CE618]
Firmware: [81.00B81]
SMART Available: yes
Quirks:
Awake: yes
SMART Disk Health Good: yes
Off-line Data Collection Status: [Off-line data collection activity was never started.]
Total Time To Complete Off-Line Data Collection: 41040 s
Self-Test Execution Status: [The previous self-test routine completed without error or no self-test has ever been run.]
Percent Self-Test Remaining: 0%
Conveyance Self-Test Available: no
Short/Extended Self-Test Available: yes
Start Self-Test Available: yes
Abort Self-Test Available: yes
Short Self-Test Polling Time: 2 min
Extended Self-Test Polling Time: 436 min
Conveyance Self-Test Polling Time: 0 min
Bad Sectors: 0 sectors
Powered On: 9.8 months
Power Cycles: 32
Average Powered On Per Power Cycle: 9.2 days
Temperature: 33.0 C
Attribute Parsing Verification: Good
Overall Status: GOOD
ID# Name                        Value Worst Thres Pretty      Raw            Type    Updates Good Good/Past
  1 raw-read-error-rate         200   200    51   0           0x000000000000 prefail online  yes  yes
  3 spin-up-time                229   223    21   3.5 s       0xcd0d00000000 prefail online  yes  yes
  4 start-stop-count            100   100     0   32          0x200000000000 old-age online  n/a  n/a
  5 reallocated-sector-count    200   200   140   0 sectors   0x000000000000 prefail online  yes  yes
  7 seek-error-rate             100   253     0   0           0x000000000000 old-age online  n/a  n/a
  9 power-on-hours               91    91     0   9.8 months  0x861b00000000 old-age online  n/a  n/a
 10 spin-retry-count            100   253     0   0           0x000000000000 old-age online  n/a  n/a
 11 calibration-retry-count     100   253     0   0           0x000000000000 old-age online  n/a  n/a
12 power-cycle-count           100   100     0   32          0x200000000000 old-age online  n/a  n/a
192 power-off-retract-count     200   200     0   18          0x120000000000 old-age online  n/a  n/a
193 load-cycle-count            200   200     0   1791        0xff0600000000 old-age online  n/a  n/a
194 temperature-celsius-2       117   105     0   33.0 C      0x210000000000 old-age online  n/a  n/a
196 reallocated-event-count     200   200     0   0           0x000000000000 old-age online  n/a  n/a
197 current-pending-sector      200   200     0   0 sectors   0x000000000000 old-age online  n/a  n/a
198 offline-uncorrectable       100   253     0   0 sectors   0x000000000000 old-age offline n/a  n/a
199 udma-crc-error-count        200   200     0   0           0x000000000000 old-age online  n/a  n/a
200 multi-zone-error-rate       100   253     0   0           0x000000000000 old-age offline n/a  n/a
```

特にディスクに問題は無さそうなので、clear & scrubをして様子を見る。

```sh
$ sudo zpool clear tank
$ sudo zpool scrub tank
```

備考: そもそも明示的にscrubを実行した覚えがないので理由を調べたら、どうやらUbuntuに勝手に設定されていた ([参考1](https://www.reddit.com/r/zfs/comments/9bxj2y/where_is_the_scrub_schedule_coming_from_on_ubuntu/), [参考2](https://github.com/openzfs/zfs/issues/9858))

```sh
$ cat /etc/cron.d/zfsutils-linux 
PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin

# TRIM the first Sunday of every month.
24 0 1-7 * * root if [ $(date +\%w) -eq 0 ] && [ -x /usr/lib/zfs-linux/trim ]; then /usr/lib/zfs-linux/trim; fi

# Scrub the second Sunday of every month.
24 0 8-14 * * root if [ $(date +\%w) -eq 0 ] && [ -x /usr/lib/zfs-linux/scrub ]; then /usr/lib/zfs-linux/scrub; fi
```

### 2022-06-26

外部USB接続のHDDにzpoolを作成する。

```sh
$ sudo zpool create tank-mirror usb-WDC_WUH_721816ALE6L4_000000004899-0:0
```

`zfs send`を使ってメインのzpoolからバックアップを取る。

```sh
#!/bin/bash

set -ex

d=$(date +%F)
sudo zfs snapshot tank/photos@$d
sudo zfs send tank/photos@$d | sudo zfs receive tank-mirror/photos
```

備考: 自動でマウントされない場合は以下を実行する。

```sh
$ sudo zpool import tank-mirror 
```

### 2022-02-03

`WD Red Plus 4 TB, 3.5", CMR` * 8 を使用してRAID-Z2でzpoolを作成する。

```sh
#!/bin/bash

$ sudo zpool create tank raidz2 \
  /dev/disk/by-id/wwn-0x50014ee214768e83 \
  /dev/disk/by-id/wwn-0x50014ee269cb8402 \
  /dev/disk/by-id/wwn-0x50014ee269cb9a72 \
  /dev/disk/by-id/wwn-0x50014ee269cba7a6 \
  /dev/disk/by-id/wwn-0x50014ee269cbc494 \
  /dev/disk/by-id/wwn-0x50014ee2bf218152 \
  /dev/disk/by-id/wwn-0x50014ee2bf21c67f \
  /dev/disk/by-id/wwn-0x50014ee2bf21c772
```

ディスクはIDで指定するのが再現性があってよい。

備考: `zpool history`でコマンド履歴が見られる。

```sh
$ sudo zpool history tank
History for 'tank':
2022-02-02.15:29:47 zpool create tank raidz2 /dev/disk/by-id/wwn-0x50014ee2bf21c772 /dev/disk/by-id/wwn-0x50014ee2bf218152 /dev/disk/by-id/wwn-0x50014ee269cb9a72 /dev/disk/by-id/wwn-0x50014ee214768e83 /dev/disk/by-id/wwn-0x50014ee269cba7a6 /dev/disk/by-id/wwn-0x50014ee269cbc494 /dev/disk/by-id/wwn-0x50014ee269cb8402 /dev/disk/by-id/wwn-0x50014ee2bf21c67f
2022-02-02.15:31:04 zfs create tank/test
2022-02-03.15:21:18 zpool import -c /etc/zfs/zpool.cache -aN
2022-02-03.15:46:43 zfs destroy tank/test
2022-02-03.15:47:25 zfs create tank/photos
2022-02-06.15:18:28 zpool import -c /etc/zfs/zpool.cache -aN
2022-02-13.00:24:28 zpool scrub tank
2022-02-18.12:43:53 zpool import -c /etc/zfs/zpool.cache -aN
2022-02-18.13:11:34 zpool import -c /etc/zfs/zpool.cache -aN
2022-03-07.00:23:50 zpool import -c /etc/zfs/zpool.cache -aN
2022-03-08.21:37:37 zpool import -c /etc/zfs/zpool.cache -aN
2022-03-13.00:24:28 zpool scrub tank
2022-04-09.18:19:06 zpool import -c /etc/zfs/zpool.cache -aN
2022-04-10.00:24:20 zpool scrub tank
2022-05-08.00:24:23 zpool scrub tank
2022-05-15.16:09:56 zpool import -c /etc/zfs/zpool.cache -aN
2022-05-22.11:01:41 zfs snapshot tank@2022-05-22
2022-05-22.11:13:59 zfs destroy tank@2022-05-22
2022-05-22.11:15:53 zfs snapshot tank/photos@2022-05-22
2022-06-01.20:24:24 zpool import -c /etc/zfs/zpool.cache -aN
2022-06-12.00:24:29 zpool scrub tank
2022-06-19.11:53:14 zpool import -c /etc/zfs/zpool.cache -aN
2022-07-01.14:37:38 zpool import -c /etc/zfs/zpool.cache -aN
2022-07-10.00:24:20 zpool scrub tank
2022-07-15.16:20:11 zpool import -c /etc/zfs/zpool.cache -aN
2022-08-08.04:39:29 zpool import -c /etc/zfs/zpool.cache -aN
2022-08-14.00:24:09 zpool scrub tank
2022-09-19.17:43:55 zpool import -c /etc/zfs/zpool.cache -aN
2022-10-02.18:31:31 zpool import -c /etc/zfs/zpool.cache -aN
2022-10-09.17:37:56 zpool import -c /etc/zfs/zpool.cache -aN
2022-10-26.14:15:12 zpool import -c /etc/zfs/zpool.cache -aN
2022-11-13.00:24:09 zpool scrub tank
2022-12-12.11:10:29 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-12.11:13:46 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.07:52:28 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.09:51:16 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.10:10:05 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.18:11:33 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.18:27:52 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.18:39:38 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.18:57:11 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.18:59:52 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.19:06:14 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.19:09:40 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-17.19:16:47 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-18.09:58:24 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-18.14:58:45 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-18.16:25:26 zpool import -c /etc/zfs/zpool.cache -aN
2022-12-28.16:06:45 zpool import -c /etc/zfs/zpool.cache -aN
```
